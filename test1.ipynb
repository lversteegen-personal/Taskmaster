{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Only run in colab\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "%cd /content/drive/MyDrive/Github/Taskmaster\n",
    "#!pip install tensorflow --upgrade\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\leove\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\leove\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import small_rubiks as rubiks\n",
    "import numpy as np\n",
    "from student import student\n",
    "from classroom import classroom\n",
    "from teacher import teacher\n",
    "from small_rubiks_neural_networks import student_network\n",
    "\n",
    "task = rubiks.rubiks_task\n",
    "setup = rubiks.rubiks_setup\n",
    "rng = np.random.default_rng(seed=0)\n",
    "\n",
    "#my_student_network = student_network.load(\"models/trained.h5\")\n",
    "my_student_network = student_network.create(residual_layers = 4)\n",
    "student_template = student(task, 50,my_student_network)\n",
    "t = teacher(setup, lambda n : 1+rng.poisson(lam=3,size=n))\n",
    "c = classroom(task, setup, t, student_template, n_students=1, max_steps=10)\n",
    "#c.run_training_batch(n_problems=1,epochs_per_episode=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished step 0, 1 out of 1 remain open.\n",
      "Finished step 1, 1 out of 1 remain open.\n",
      "Finished step 2, 1 out of 1 remain open.\n",
      "Finished step 3, 1 out of 1 remain open.\n",
      "Finished step 4, 1 out of 1 remain open.\n",
      "Finished step 5, 1 out of 1 remain open.\n",
      "Finished step 6, 1 out of 1 remain open.\n",
      "Finished step 7, 1 out of 1 remain open.\n",
      "Finished step 8, 1 out of 1 remain open.\n",
      "Finished step 9, 1 out of 1 remain open.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'student_network' object has no attribute 'fit'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m):\n\u001b[1;32m----> 2\u001b[0m     \u001b[43mc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_training_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_problems\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mepochs_per_episode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;66;03m#if i % 10 == 0:\u001b[39;00m\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;66;03m#    my_student_network.save(\"models/t.keras\")\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\leove\\OneDrive\\Documents\\Selberprogrammiertes\\Taskmaster\\classroom.py:57\u001b[0m, in \u001b[0;36mclassroom.run_training_batch\u001b[1;34m(self, n_problems, epochs_per_episode)\u001b[0m\n\u001b[0;32m     54\u001b[0m evals \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(evals)\n\u001b[0;32m     55\u001b[0m policies \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(policies)\n\u001b[1;32m---> 57\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstudent\u001b[39m.\u001b[39;49mneural_network\u001b[39m.\u001b[39;49mfit(inputs,evals,policies,epochs_per_episode)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'student_network' object has no attribute 'fit'"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    c.run_training_batch(n_problems=1,epochs_per_episode=3)\n",
    "    #if i % 10 == 0:\n",
    "    #    my_student_network.save(\"models/t.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\leove\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\leove\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:From C:\\Users\\leove\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "120/120 [==============================] - 4s 10ms/step - loss: 2.6523 - val_loss: 1.9639\n",
      "Epoch 2/10\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 1.5063 - val_loss: 1.2337\n",
      "Epoch 3/10\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.9695 - val_loss: 0.8585\n",
      "Epoch 4/10\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.7035 - val_loss: 0.6694\n",
      "Epoch 5/10\n",
      "120/120 [==============================] - 1s 9ms/step - loss: 0.5581 - val_loss: 0.5641\n",
      "Epoch 6/10\n",
      "120/120 [==============================] - 1s 9ms/step - loss: 0.4779 - val_loss: 0.5015\n",
      "Epoch 7/10\n",
      "120/120 [==============================] - 1s 10ms/step - loss: 0.4370 - val_loss: 0.4718\n",
      "Epoch 8/10\n",
      "120/120 [==============================] - 1s 9ms/step - loss: 0.4127 - val_loss: 0.4546\n",
      "Epoch 9/10\n",
      "120/120 [==============================] - 1s 9ms/step - loss: 0.3980 - val_loss: 0.4511\n",
      "Epoch 10/10\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.3859 - val_loss: 0.4351\n",
      "Epoch 1/10\n",
      "120/120 [==============================] - 1s 9ms/step - loss: 0.4028 - val_loss: 0.4053\n",
      "Epoch 2/10\n",
      "120/120 [==============================] - 1s 9ms/step - loss: 0.3798 - val_loss: 0.4030\n",
      "Epoch 3/10\n",
      "120/120 [==============================] - 1s 9ms/step - loss: 0.3699 - val_loss: 0.4051\n",
      "Epoch 4/10\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.3598 - val_loss: 0.3995\n",
      "Epoch 5/10\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.3531 - val_loss: 0.3951\n",
      "Epoch 6/10\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.3456 - val_loss: 0.3901\n",
      "Epoch 7/10\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.3386 - val_loss: 0.3783\n",
      "Epoch 8/10\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.3344 - val_loss: 0.3815\n",
      "Epoch 9/10\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.3284 - val_loss: 0.3853\n",
      "Epoch 10/10\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.3231 - val_loss: 0.3791\n",
      "Epoch 1/10\n",
      "120/120 [==============================] - 1s 9ms/step - loss: 0.3471 - val_loss: 0.3704\n",
      "Epoch 2/10\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.3258 - val_loss: 0.3511\n",
      "Epoch 3/10\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.3179 - val_loss: 0.3640\n",
      "Epoch 4/10\n",
      "120/120 [==============================] - 1s 9ms/step - loss: 0.3127 - val_loss: 0.3518\n",
      "Epoch 5/10\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.3066 - val_loss: 0.3525\n",
      "Epoch 6/10\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.3017 - val_loss: 0.3520\n",
      "Epoch 7/10\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.2966 - val_loss: 0.3405\n",
      "Epoch 8/10\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.2933 - val_loss: 0.3527\n",
      "Epoch 9/10\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.2886 - val_loss: 0.3510\n",
      "Epoch 10/10\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.2860 - val_loss: 0.3368\n",
      "Epoch 1/10\n",
      "120/120 [==============================] - 1s 9ms/step - loss: 0.3129 - val_loss: 0.3210\n",
      "Epoch 2/10\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.2933 - val_loss: 0.3168\n",
      "Epoch 3/10\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.2858 - val_loss: 0.3129\n",
      "Epoch 4/10\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.2829 - val_loss: 0.3281\n",
      "Epoch 5/10\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.2771 - val_loss: 0.3377\n",
      "Epoch 6/10\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.2755 - val_loss: 0.3181\n",
      "Epoch 7/10\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.2712 - val_loss: 0.3178\n",
      "Epoch 8/10\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.2691 - val_loss: 0.3307\n",
      "Epoch 9/10\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.2665 - val_loss: 0.3170\n",
      "Epoch 10/10\n",
      "120/120 [==============================] - 1s 9ms/step - loss: 0.2653 - val_loss: 0.3185\n",
      "Epoch 1/10\n",
      "120/120 [==============================] - 1s 9ms/step - loss: 0.2931 - val_loss: 0.3281\n",
      "Epoch 2/10\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.2742 - val_loss: 0.3280\n",
      "Epoch 3/10\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.2714 - val_loss: 0.3554\n",
      "Epoch 4/10\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.2644 - val_loss: 0.3150\n",
      "Epoch 5/10\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.2613 - val_loss: 0.3075\n",
      "Epoch 6/10\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.2581 - val_loss: 0.3095\n",
      "Epoch 7/10\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.2566 - val_loss: 0.3269\n",
      "Epoch 8/10\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.2533 - val_loss: 0.3057\n",
      "Epoch 9/10\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.2513 - val_loss: 0.3264\n",
      "Epoch 10/10\n",
      "120/120 [==============================] - 1s 9ms/step - loss: 0.2497 - val_loss: 0.3366\n",
      "Epoch 1/10\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.2789 - val_loss: 0.3184\n",
      "Epoch 2/10\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.2605 - val_loss: 0.3034\n",
      "Epoch 3/10\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.2543 - val_loss: 0.2971\n",
      "Epoch 4/10\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.2505 - val_loss: 0.3075\n",
      "Epoch 5/10\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.2472 - val_loss: 0.3022\n",
      "Epoch 6/10\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.2454 - val_loss: 0.3047\n",
      "Epoch 7/10\n",
      "120/120 [==============================] - 1s 9ms/step - loss: 0.2441 - val_loss: 0.2864\n",
      "Epoch 8/10\n",
      "120/120 [==============================] - 1s 9ms/step - loss: 0.2426 - val_loss: 0.3187\n",
      "Epoch 9/10\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.2390 - val_loss: 0.3376\n",
      "Epoch 10/10\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.2369 - val_loss: 0.3084\n",
      "Epoch 1/10\n",
      "120/120 [==============================] - 1s 9ms/step - loss: 0.2660 - val_loss: 0.3073\n",
      "Epoch 2/10\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.2485 - val_loss: 0.2933\n",
      "Epoch 3/10\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.2413 - val_loss: 0.2939\n",
      "Epoch 4/10\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.2383 - val_loss: 0.2958\n",
      "Epoch 5/10\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.2342 - val_loss: 0.2963\n",
      "Epoch 6/10\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.2326 - val_loss: 0.3614\n",
      "Epoch 7/10\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.2323 - val_loss: 0.3165\n",
      "Epoch 8/10\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.2282 - val_loss: 0.2977\n",
      "Epoch 9/10\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.2242 - val_loss: 0.2989\n",
      "Epoch 10/10\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.2218 - val_loss: 0.3050\n",
      "Epoch 1/10\n",
      "120/120 [==============================] - 1s 9ms/step - loss: 0.2546 - val_loss: 0.3117\n",
      "Epoch 2/10\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.2342 - val_loss: 0.2844\n",
      "Epoch 3/10\n",
      "120/120 [==============================] - 1s 9ms/step - loss: 0.2275 - val_loss: 0.2851\n",
      "Epoch 4/10\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.2247 - val_loss: 0.2847\n",
      "Epoch 5/10\n",
      "120/120 [==============================] - 1s 9ms/step - loss: 0.2219 - val_loss: 0.3048\n",
      "Epoch 6/10\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.2188 - val_loss: 0.3049\n",
      "Epoch 7/10\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.2141 - val_loss: 0.3596\n",
      "Epoch 8/10\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.2154 - val_loss: 0.2889\n",
      "Epoch 9/10\n",
      "120/120 [==============================] - 1s 9ms/step - loss: 0.2124 - val_loss: 0.2851\n",
      "Epoch 10/10\n",
      "120/120 [==============================] - 1s 9ms/step - loss: 0.2107 - val_loss: 0.2742\n",
      "Epoch 1/10\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.2423 - val_loss: 0.2585\n",
      "Epoch 2/10\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.2202 - val_loss: 0.2848\n",
      "Epoch 3/10\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.2141 - val_loss: 0.2776\n",
      "Epoch 4/10\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.2110 - val_loss: 0.2826\n",
      "Epoch 5/10\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.2096 - val_loss: 0.2818\n",
      "Epoch 6/10\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.2061 - val_loss: 0.2646\n",
      "Epoch 7/10\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.2033 - val_loss: 0.2950\n",
      "Epoch 8/10\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.2016 - val_loss: 0.2912\n",
      "Epoch 9/10\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.2017 - val_loss: 0.2696\n",
      "Epoch 10/10\n",
      "120/120 [==============================] - 1s 9ms/step - loss: 0.2000 - val_loss: 0.3102\n",
      "Epoch 1/10\n",
      "120/120 [==============================] - 1s 9ms/step - loss: 0.2319 - val_loss: 0.2748\n",
      "Epoch 2/10\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.2099 - val_loss: 0.2554\n",
      "Epoch 3/10\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.2030 - val_loss: 0.2804\n",
      "Epoch 4/10\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.2025 - val_loss: 0.3349\n",
      "Epoch 5/10\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.1983 - val_loss: 0.2610\n",
      "Epoch 6/10\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.1949 - val_loss: 0.2751\n",
      "Epoch 7/10\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.1944 - val_loss: 0.2525\n",
      "Epoch 8/10\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.1918 - val_loss: 0.2864\n",
      "Epoch 9/10\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.1917 - val_loss: 0.2709\n",
      "Epoch 10/10\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.1893 - val_loss: 0.2624\n",
      "Epoch 1/10\n",
      "120/120 [==============================] - 1s 9ms/step - loss: 0.2242 - val_loss: 0.2528\n",
      "Epoch 2/10\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.2026 - val_loss: 0.2494\n",
      "Epoch 3/10\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.1965 - val_loss: 0.2458\n",
      "Epoch 4/10\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.1919 - val_loss: 0.2712\n",
      "Epoch 5/10\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.1923 - val_loss: 0.2632\n",
      "Epoch 6/10\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.1900 - val_loss: 0.2655\n",
      "Epoch 7/10\n",
      "120/120 [==============================] - 1s 9ms/step - loss: 0.1882 - val_loss: 0.2487\n",
      "Epoch 8/10\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.1863 - val_loss: 0.2456\n",
      "Epoch 9/10\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.1854 - val_loss: 0.2609\n",
      "Epoch 10/10\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.1828 - val_loss: 0.2405\n",
      "Epoch 1/10\n",
      "120/120 [==============================] - 1s 9ms/step - loss: 0.2157 - val_loss: 0.2540\n",
      "Epoch 2/10\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.1948 - val_loss: 0.2370\n",
      "Epoch 3/10\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.1870 - val_loss: 0.2399\n",
      "Epoch 4/10\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.1860 - val_loss: 0.2562\n",
      "Epoch 5/10\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.1830 - val_loss: 0.2426\n",
      "Epoch 6/10\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.1820 - val_loss: 0.2376\n",
      "Epoch 7/10\n",
      "120/120 [==============================] - 1s 9ms/step - loss: 0.1811 - val_loss: 0.2462\n",
      "Epoch 8/10\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.1810 - val_loss: 0.2549\n",
      "Epoch 9/10\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.1803 - val_loss: 0.2509\n",
      "Epoch 10/10\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.1788 - val_loss: 0.2204\n",
      "Epoch 1/10\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.2093 - val_loss: 0.2549\n",
      "Epoch 2/10\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.1910 - val_loss: 0.2233\n",
      "Epoch 3/10\n",
      "120/120 [==============================] - 1s 9ms/step - loss: 0.1823 - val_loss: 0.2161\n",
      "Epoch 4/10\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.1813 - val_loss: 0.2335\n",
      "Epoch 5/10\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.1812 - val_loss: 0.2277\n",
      "Epoch 6/10\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.1790 - val_loss: 0.2298\n",
      "Epoch 7/10\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.1772 - val_loss: 0.2172\n",
      "Epoch 8/10\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.1774 - val_loss: 0.2236\n",
      "Epoch 9/10\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.1789 - val_loss: 0.2303\n",
      "Epoch 10/10\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.1773 - val_loss: 0.2253\n",
      "Epoch 1/10\n",
      "120/120 [==============================] - 1s 9ms/step - loss: 0.2034 - val_loss: 0.2388\n",
      "Epoch 2/10\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.1858 - val_loss: 0.2274\n",
      "Epoch 3/10\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.1798 - val_loss: 0.2270\n",
      "Epoch 4/10\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.1761 - val_loss: 0.2344\n",
      "Epoch 5/10\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.1757 - val_loss: 0.2165\n",
      "Epoch 6/10\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.1773 - val_loss: 0.2483\n",
      "Epoch 7/10\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.1734 - val_loss: 0.2312\n",
      "Epoch 8/10\n",
      "120/120 [==============================] - 1s 10ms/step - loss: 0.1732 - val_loss: 0.2407\n",
      "Epoch 9/10\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.1724 - val_loss: 0.2664\n",
      "Epoch 10/10\n",
      "120/120 [==============================] - 1s 9ms/step - loss: 0.1716 - val_loss: 0.2322\n",
      "Epoch 1/10\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.2012 - val_loss: 0.2345\n",
      "Epoch 2/10\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.1832 - val_loss: 0.2155\n",
      "Epoch 3/10\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.1782 - val_loss: 0.2034\n",
      "Epoch 4/10\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.1708 - val_loss: 0.2289\n",
      "Epoch 5/10\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.1705 - val_loss: 0.2271\n",
      "Epoch 6/10\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.1728 - val_loss: 0.2248\n",
      "Epoch 7/10\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.1723 - val_loss: 0.2565\n",
      "Epoch 8/10\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.1705 - val_loss: 0.2148\n",
      "Epoch 9/10\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.1677 - val_loss: 0.2234\n",
      "Epoch 10/10\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.1659 - val_loss: 0.2271\n"
     ]
    }
   ],
   "source": [
    "from small_rubiks_neural_networks import student_network\n",
    "import numpy as np\n",
    "import small_rubiks as rubiks\n",
    "from utils import dotdict\n",
    "from keras import regularizers\n",
    "\n",
    "params = dotdict({\n",
    "    \"residual_weights_reg\" : regularizers.l2(l2=0.001),\n",
    "    \"residual_bias_reg\" : regularizers.l2(0.001),\n",
    "    \"relu_leak\" : 0.1,\n",
    "    \"residual_units\" : 2000,\n",
    "    \"learning_rate\" : 0.001,\n",
    "    \"residual_layers\" : 0\n",
    "})\n",
    "\n",
    "my_student_network = student_network()\n",
    "my_student_network.build_state_network(params)\n",
    "\n",
    "n=4096\n",
    "rng = np.random.default_rng(seed=0)\n",
    "states = np.arange(24)[None,:].repeat(n,axis=0)\n",
    "\n",
    "for k in range(15):\n",
    "    for i in range(30):\n",
    "        _, states = rubiks.task_action(states,rng.choice(a=12,size=n))\n",
    "\n",
    "    actions = rng.choice(12,size=n)\n",
    "    _, next_states = rubiks.task_action(states,actions)\n",
    "    my_student_network.fit_state(states,actions,next_states,epochs = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  BY    \n",
      "  OO    \n",
      "RBWWGBOY\n",
      "GRGBWWGY\n",
      "  YR    \n",
      "  OR    \n",
      "\n",
      "  WY    \n",
      "  GO    \n",
      "BRYWGBOO\n",
      "RGOBWWGB\n",
      "  YR    \n",
      "  YR    \n",
      "\n",
      "  YY    \n",
      "  YO    \n",
      "GRBWGBOO\n",
      "RBOBWWGY\n",
      "  WR    \n",
      "  GR    \n",
      "\n",
      "  BW    \n",
      "  OB    \n",
      "RBWRWGOY\n",
      "GRGRWBYY\n",
      "  YG    \n",
      "  OO    \n",
      "\n",
      "  BG    \n",
      "  OO    \n",
      "RBWYBWRY\n",
      "GRGOGWRY\n",
      "  YW    \n",
      "  OB    \n",
      "\n",
      "  BY    \n",
      "  OO    \n",
      "RBWWGBOY\n",
      "GYGRGBWW\n",
      "  OY    \n",
      "  RR    \n",
      "\n",
      "  BY    \n",
      "  OO    \n",
      "RBWWGBOY\n",
      "GBWWGYGR\n",
      "  RR    \n",
      "  YO    \n",
      "\n",
      "  YO    \n",
      "  BO    \n",
      "OYRBWWGB\n",
      "GRGBWWGY\n",
      "  YR    \n",
      "  OR    \n",
      "\n",
      "  OB    \n",
      "  OY    \n",
      "WWGBOYRB\n",
      "GRGBWWGY\n",
      "  YR    \n",
      "  OR    \n",
      "\n",
      "  BY    \n",
      "  GW    \n",
      "ROWBRBOY\n",
      "GOWGYWGY\n",
      "  BR    \n",
      "  OR    \n",
      "\n",
      "  BY    \n",
      "  RB    \n",
      "RYGWOBOY\n",
      "GRBWOWGY\n",
      "  WG    \n",
      "  OR    \n",
      "\n",
      "  BW    \n",
      "  OO    \n",
      "YBWWGRGO\n",
      "BRGBWOYY\n",
      "  YR    \n",
      "  RG    \n",
      "\n",
      "  GR    \n",
      "  OO    \n",
      "OBWWGBYY\n",
      "RRGBWYOG\n",
      "  YR    \n",
      "  WB    \n",
      "\n"
     ]
    }
   ],
   "source": [
    "rubiks.print_coloring(states[0])\n",
    "for i in range(12):\n",
    "    rubiks.print_coloring(rubiks.task_action(states[0],i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 26ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[0, 1, 2, 0, 0, 5, 0, 5, 3, 0, 4, 3, 1, 2, 4, 1, 2, 5, 1, 5, 2, 1,\n",
       "         4, 3]], dtype=int64),\n",
       " array([0, 5, 2, 0, 5, 3, 0, 4, 2, 0, 4, 3, 1, 2, 4, 1, 2, 5, 1, 3, 4, 1,\n",
       "        3, 5]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states = np.arange(24)[None,:]\n",
    "actions = np.array([0])\n",
    "prediction = my_student_network.predict_state(states,actions)\n",
    "prediction.reshape(1,24,6).argmax(axis=2), rubiks.start_coloring[rubiks.task_action(states,actions)[1][0]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ea0df938a2fb4e996541c96a685bf803a61ab029232d990df5cc8c7e24c9f388"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
