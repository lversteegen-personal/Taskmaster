{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Only run in colab\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "%cd /content/drive/MyDrive/Github/Taskmaster\n",
    "#!pip install tensorflow --upgrade\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\leove\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\leove\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import small_rubiks as rubiks\n",
    "import numpy as np\n",
    "from student import student\n",
    "from classroom import classroom\n",
    "from teacher import teacher\n",
    "from small_rubiks_neural_networks import student_network\n",
    "from utils import dotdict\n",
    "from keras import regularizers\n",
    "\n",
    "task = rubiks.rubiks_task\n",
    "setup = rubiks.rubiks_setup\n",
    "rng = np.random.default_rng(seed=0)\n",
    "\n",
    "params = dotdict({\n",
    "            \"residual_weights_reg\" : regularizers.l2(0.001),\n",
    "            \"residual_bias_reg\" : regularizers.l2(0.01),\n",
    "            \"relu_leak\" : 0.1,\n",
    "            \"residual_units\" : 200,\n",
    "            \"learning_rate\" : 0.001,\n",
    "            \"residual_layers\" : 3\n",
    "})\n",
    "my_student_network = student_network.create(params)\n",
    "#my_student_network = student_network.load_value_network(\"models/trained.h5\")\n",
    "student_template = student(task, 50,my_student_network)\n",
    "t = teacher(setup, lambda n : 1*np.ones(n,dtype=int))##1+rng.poisson(lam=3,size=n)\n",
    "c = classroom(task, setup, t, student_template, n_students=1, max_steps=10, buffer_size = lambda n : 2048)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.max_steps=5\n",
    "t.step_dist = lambda n : 3*np.ones(n,dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 0.\n",
      "Before step 1, 100 out of 100 remain open.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'depth'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [3], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRound \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m     \u001b[43mc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_training_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_problems\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mepochs_per_episode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;66;03m#if i % 10 == 9:\u001b[39;00m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;66;03m#    my_student_network.save(\"models/trained.h5\")\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\leove\\OneDrive\\Documents\\Selberprogrammiertes\\Taskmaster\\classroom.py:43\u001b[0m, in \u001b[0;36mclassroom.run_training_batch\u001b[1;34m(self, n_problems, epochs_per_episode)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun_training_batch\u001b[39m(\u001b[39mself\u001b[39m, n_problems, epochs_per_episode):\n\u001b[0;32m     42\u001b[0m     problems \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mteacher\u001b[39m.\u001b[39mgenerate_problems(n_problems)\n\u001b[1;32m---> 43\u001b[0m     replay_record, proof_nodes \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtest_students(problems)\n\u001b[0;32m     45\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtotal_tasks \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m n_problems\n\u001b[0;32m     46\u001b[0m     inputs \u001b[39m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\leove\\OneDrive\\Documents\\Selberprogrammiertes\\Taskmaster\\classroom.py:106\u001b[0m, in \u001b[0;36mclassroom.test_students\u001b[1;34m(self, start_states)\u001b[0m\n\u001b[0;32m    103\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBefore step \u001b[39m\u001b[39m{\u001b[39;00ms\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(unfinished_task_indices)\u001b[39m}\u001b[39;00m\u001b[39m out of \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(task_nodes)\u001b[39m}\u001b[39;00m\u001b[39m remain open.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 106\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstudent\u001b[39m.\u001b[39;49mrun_action_step([task_nodes[i] \u001b[39mfor\u001b[39;49;00m i \u001b[39min\u001b[39;49;00m unfinished_task_indices])\n\u001b[0;32m    108\u001b[0m \u001b[39mfor\u001b[39;00m j, (action, pi, eval_root) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(result):\n\u001b[0;32m    110\u001b[0m     k \u001b[39m=\u001b[39m unfinished_task_indices[j]\n",
      "File \u001b[1;32mc:\\Users\\leove\\OneDrive\\Documents\\Selberprogrammiertes\\Taskmaster\\student.py:62\u001b[0m, in \u001b[0;36mstudent.run_action_step\u001b[1;34m(self, task_nodes)\u001b[0m\n\u001b[0;32m     58\u001b[0m eval_trees \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m(initial_eval_trees)\n\u001b[0;32m     60\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_sim):\n\u001b[1;32m---> 62\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun_simulation_step(eval_trees)\n\u001b[0;32m     63\u001b[0m     eval_trees\u001b[39m.\u001b[39mdifference_update([e \u001b[39mfor\u001b[39;00m e \u001b[39min\u001b[39;00m eval_trees \u001b[39mif\u001b[39;00m e\u001b[39m.\u001b[39mcompleted])\n\u001b[0;32m     65\u001b[0m result_list \u001b[39m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\leove\\OneDrive\\Documents\\Selberprogrammiertes\\Taskmaster\\student.py:43\u001b[0m, in \u001b[0;36mstudent.run_simulation_step\u001b[1;34m(self, eval_trees)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[39mfor\u001b[39;00m i, t \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(eval_trees):\n\u001b[0;32m     42\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m t\u001b[39m.\u001b[39mcompleted:\n\u001b[1;32m---> 43\u001b[0m         t\u001b[39m.\u001b[39;49mupdate(eval_leaves[i])\n",
      "File \u001b[1;32mc:\\Users\\leove\\OneDrive\\Documents\\Selberprogrammiertes\\Taskmaster\\evaluation_tree.py:112\u001b[0m, in \u001b[0;36mevaluation_tree_node.update\u001b[1;34m(self, task_node)\u001b[0m\n\u001b[0;32m    109\u001b[0m         \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m    111\u001b[0m \u001b[39mif\u001b[39;00m task_node\u001b[39m.\u001b[39mcompleted:\n\u001b[1;32m--> 112\u001b[0m     v \u001b[39m=\u001b[39m task_node\u001b[39m.\u001b[39;49mtask\u001b[39m.\u001b[39;49mreward_function(task_node\u001b[39m.\u001b[39;49mstate,task_node\u001b[39m.\u001b[39;49mdepth\u001b[39m-\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtask_node\u001b[39m.\u001b[39;49mdepth)\n\u001b[0;32m    113\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    114\u001b[0m     v \u001b[39m=\u001b[39m task_node\u001b[39m.\u001b[39minitial_evaluation\n",
      "File \u001b[1;32mc:\\Users\\leove\\OneDrive\\Documents\\Selberprogrammiertes\\Taskmaster\\small_rubiks.py:242\u001b[0m, in \u001b[0;36mreward_function\u001b[1;34m(state, steps)\u001b[0m\n\u001b[0;32m    239\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mreward_function\u001b[39m(state, steps):\n\u001b[0;32m    241\u001b[0m     \u001b[39mif\u001b[39;00m check_win(state):\n\u001b[1;32m--> 242\u001b[0m         \u001b[39mreturn\u001b[39;00m gamma\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m(steps\u001b[39m-\u001b[39mstate\u001b[39m.\u001b[39;49mdepth)\n\u001b[0;32m    243\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    244\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39m0\u001b[39m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'depth'"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(10):\n",
    "    print(f\"Round {i}.\")\n",
    "    c.run_training_batch(n_problems=100,epochs_per_episode=3)\n",
    "    #if i % 10 == 9:\n",
    "    #    my_student_network.save(\"models/trained.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "120/120 [==============================] - 3s 6ms/step - loss: 0.6253 - val_loss: 0.4650\n",
      "Epoch 2/10\n",
      "120/120 [==============================] - 0s 4ms/step - loss: 0.4461 - val_loss: 0.4431\n",
      "Epoch 3/10\n",
      "120/120 [==============================] - 0s 4ms/step - loss: 0.4259 - val_loss: 0.4346\n",
      "Epoch 4/10\n",
      "120/120 [==============================] - 0s 3ms/step - loss: 0.4116 - val_loss: 0.4288\n",
      "Epoch 5/10\n",
      "120/120 [==============================] - 0s 4ms/step - loss: 0.4003 - val_loss: 0.4252\n",
      "Epoch 6/10\n",
      "120/120 [==============================] - 0s 4ms/step - loss: 0.3905 - val_loss: 0.4197\n",
      "Epoch 7/10\n",
      "120/120 [==============================] - 0s 4ms/step - loss: 0.3819 - val_loss: 0.4166\n",
      "Epoch 8/10\n",
      "120/120 [==============================] - 0s 4ms/step - loss: 0.3740 - val_loss: 0.4122\n",
      "Epoch 9/10\n",
      "120/120 [==============================] - 0s 4ms/step - loss: 0.3671 - val_loss: 0.4092\n",
      "Epoch 10/10\n",
      "120/120 [==============================] - 0s 3ms/step - loss: 0.3602 - val_loss: 0.4064\n",
      "Epoch 1/10\n",
      "120/120 [==============================] - 0s 4ms/step - loss: 0.3987 - val_loss: 0.4006\n",
      "Epoch 2/10\n",
      "120/120 [==============================] - 0s 3ms/step - loss: 0.3767 - val_loss: 0.3878\n",
      "Epoch 3/10\n",
      "120/120 [==============================] - 0s 4ms/step - loss: 0.3653 - val_loss: 0.3826\n",
      "Epoch 4/10\n",
      "120/120 [==============================] - 0s 4ms/step - loss: 0.3558 - val_loss: 0.3790\n",
      "Epoch 5/10\n",
      "120/120 [==============================] - 0s 3ms/step - loss: 0.3481 - val_loss: 0.3750\n",
      "Epoch 6/10\n",
      "120/120 [==============================] - 0s 3ms/step - loss: 0.3405 - val_loss: 0.3709\n",
      "Epoch 7/10\n",
      "120/120 [==============================] - 0s 4ms/step - loss: 0.3340 - val_loss: 0.3676\n",
      "Epoch 8/10\n",
      "120/120 [==============================] - 0s 4ms/step - loss: 0.3277 - val_loss: 0.3636\n",
      "Epoch 9/10\n",
      "120/120 [==============================] - 0s 4ms/step - loss: 0.3213 - val_loss: 0.3614\n",
      "Epoch 10/10\n",
      "120/120 [==============================] - 0s 3ms/step - loss: 0.3152 - val_loss: 0.3574\n",
      "Epoch 1/10\n",
      "120/120 [==============================] - 0s 4ms/step - loss: 0.3537 - val_loss: 0.3484\n",
      "Epoch 2/10\n",
      "120/120 [==============================] - 0s 4ms/step - loss: 0.3311 - val_loss: 0.3381\n",
      "Epoch 3/10\n",
      "120/120 [==============================] - 0s 4ms/step - loss: 0.3188 - val_loss: 0.3283\n",
      "Epoch 4/10\n",
      "120/120 [==============================] - 0s 3ms/step - loss: 0.3089 - val_loss: 0.3244\n",
      "Epoch 5/10\n",
      "120/120 [==============================] - 0s 3ms/step - loss: 0.3001 - val_loss: 0.3186\n",
      "Epoch 6/10\n",
      "120/120 [==============================] - 0s 3ms/step - loss: 0.2922 - val_loss: 0.3152\n",
      "Epoch 7/10\n",
      "120/120 [==============================] - 0s 4ms/step - loss: 0.2849 - val_loss: 0.3102\n",
      "Epoch 8/10\n",
      "120/120 [==============================] - 0s 4ms/step - loss: 0.2776 - val_loss: 0.3076\n",
      "Epoch 9/10\n",
      "120/120 [==============================] - 0s 4ms/step - loss: 0.2724 - val_loss: 0.3024\n",
      "Epoch 10/10\n",
      "120/120 [==============================] - 0s 3ms/step - loss: 0.2670 - val_loss: 0.3026\n",
      "Epoch 1/10\n",
      "120/120 [==============================] - 0s 4ms/step - loss: 0.3035 - val_loss: 0.2988\n",
      "Epoch 2/10\n",
      "120/120 [==============================] - 0s 4ms/step - loss: 0.2838 - val_loss: 0.2843\n",
      "Epoch 3/10\n",
      "120/120 [==============================] - 0s 4ms/step - loss: 0.2728 - val_loss: 0.2793\n",
      "Epoch 4/10\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 0.2654 - val_loss: 0.2744\n",
      "Epoch 5/10\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 0.2581 - val_loss: 0.2709\n",
      "Epoch 6/10\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 0.2524 - val_loss: 0.2689\n",
      "Epoch 7/10\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 0.2468 - val_loss: 0.2657\n",
      "Epoch 8/10\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 0.2433 - val_loss: 0.2629\n",
      "Epoch 9/10\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 0.2376 - val_loss: 0.2645\n",
      "Epoch 10/10\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 0.2340 - val_loss: 0.2600\n",
      "Epoch 1/10\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 0.2704 - val_loss: 0.2538\n",
      "Epoch 2/10\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 0.2523 - val_loss: 0.2499\n",
      "Epoch 3/10\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 0.2436 - val_loss: 0.2419\n",
      "Epoch 4/10\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 0.2366 - val_loss: 0.2391\n",
      "Epoch 5/10\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 0.2321 - val_loss: 0.2406\n",
      "Epoch 6/10\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 0.2272 - val_loss: 0.2368\n",
      "Epoch 7/10\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 0.2227 - val_loss: 0.2352\n",
      "Epoch 8/10\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 0.2191 - val_loss: 0.2354\n",
      "Epoch 9/10\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 0.2154 - val_loss: 0.2329\n",
      "Epoch 10/10\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 0.2123 - val_loss: 0.2323\n",
      "Epoch 1/10\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 0.2457 - val_loss: 0.2246\n",
      "Epoch 2/10\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 0.2293 - val_loss: 0.2184\n",
      "Epoch 3/10\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 0.2229 - val_loss: 0.2196\n",
      "Epoch 4/10\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 0.2163 - val_loss: 0.2167\n",
      "Epoch 5/10\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 0.2122 - val_loss: 0.2169\n",
      "Epoch 6/10\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 0.2082 - val_loss: 0.2109\n",
      "Epoch 7/10\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 0.2042 - val_loss: 0.2122\n",
      "Epoch 8/10\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 0.2001 - val_loss: 0.2115\n",
      "Epoch 9/10\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 0.1969 - val_loss: 0.2089\n",
      "Epoch 10/10\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 0.1938 - val_loss: 0.2086\n",
      "Epoch 1/10\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 0.2281 - val_loss: 0.2047\n",
      "Epoch 2/10\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 0.2119 - val_loss: 0.2036\n",
      "Epoch 3/10\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 0.2054 - val_loss: 0.1968\n",
      "Epoch 4/10\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 0.1996 - val_loss: 0.1958\n",
      "Epoch 5/10\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 0.1952 - val_loss: 0.1950\n",
      "Epoch 6/10\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 0.1918 - val_loss: 0.1969\n",
      "Epoch 7/10\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 0.1878 - val_loss: 0.1943\n",
      "Epoch 8/10\n",
      "120/120 [==============================] - 1s 4ms/step - loss: 0.1859 - val_loss: 0.1935\n",
      "Epoch 9/10\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 0.1833 - val_loss: 0.1914\n",
      "Epoch 10/10\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 0.1799 - val_loss: 0.1902\n",
      "Epoch 1/10\n",
      "120/120 [==============================] - 1s 6ms/step - loss: 0.2110 - val_loss: 0.1862\n",
      "Epoch 2/10\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 0.1961 - val_loss: 0.1815\n",
      "Epoch 3/10\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 0.1890 - val_loss: 0.1780\n",
      "Epoch 4/10\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 0.1841 - val_loss: 0.1823\n",
      "Epoch 5/10\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 0.1798 - val_loss: 0.1815\n",
      "Epoch 6/10\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 0.1769 - val_loss: 0.1759\n",
      "Epoch 7/10\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 0.1739 - val_loss: 0.1745\n",
      "Epoch 8/10\n",
      "120/120 [==============================] - 1s 6ms/step - loss: 0.1707 - val_loss: 0.1760\n",
      "Epoch 9/10\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.1686 - val_loss: 0.1720\n",
      "Epoch 10/10\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 0.1669 - val_loss: 0.1735\n",
      "Epoch 1/10\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 0.1971 - val_loss: 0.1683\n",
      "Epoch 2/10\n",
      "120/120 [==============================] - 1s 4ms/step - loss: 0.1840 - val_loss: 0.1649\n",
      "Epoch 3/10\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 0.1774 - val_loss: 0.1648\n",
      "Epoch 4/10\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 0.1719 - val_loss: 0.1620\n",
      "Epoch 5/10\n",
      "120/120 [==============================] - 1s 4ms/step - loss: 0.1680 - val_loss: 0.1624\n",
      "Epoch 6/10\n",
      "120/120 [==============================] - 1s 4ms/step - loss: 0.1650 - val_loss: 0.1597\n",
      "Epoch 7/10\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 0.1624 - val_loss: 0.1616\n",
      "Epoch 8/10\n",
      "120/120 [==============================] - 1s 4ms/step - loss: 0.1595 - val_loss: 0.1603\n",
      "Epoch 9/10\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 0.1573 - val_loss: 0.1593\n",
      "Epoch 10/10\n",
      "120/120 [==============================] - 1s 4ms/step - loss: 0.1552 - val_loss: 0.1574\n",
      "Epoch 1/10\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 0.1834 - val_loss: 0.1547\n",
      "Epoch 2/10\n",
      "120/120 [==============================] - 1s 4ms/step - loss: 0.1693 - val_loss: 0.1497\n",
      "Epoch 3/10\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 0.1633 - val_loss: 0.1519\n",
      "Epoch 4/10\n",
      "120/120 [==============================] - 1s 4ms/step - loss: 0.1598 - val_loss: 0.1460\n",
      "Epoch 5/10\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 0.1566 - val_loss: 0.1447\n",
      "Epoch 6/10\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 0.1528 - val_loss: 0.1475\n",
      "Epoch 7/10\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 0.1507 - val_loss: 0.1438\n",
      "Epoch 8/10\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 0.1484 - val_loss: 0.1421\n",
      "Epoch 9/10\n",
      "120/120 [==============================] - 1s 4ms/step - loss: 0.1467 - val_loss: 0.1414\n",
      "Epoch 10/10\n",
      "120/120 [==============================] - 1s 4ms/step - loss: 0.1458 - val_loss: 0.1426\n",
      "Epoch 1/10\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 0.1712 - val_loss: 0.1440\n",
      "Epoch 2/10\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 0.1597 - val_loss: 0.1392\n",
      "Epoch 3/10\n",
      "120/120 [==============================] - 0s 4ms/step - loss: 0.1537 - val_loss: 0.1387\n",
      "Epoch 4/10\n",
      "120/120 [==============================] - 1s 4ms/step - loss: 0.1487 - val_loss: 0.1370\n",
      "Epoch 5/10\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 0.1460 - val_loss: 0.1376\n",
      "Epoch 6/10\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 0.1436 - val_loss: 0.1380\n",
      "Epoch 7/10\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 0.1406 - val_loss: 0.1333\n",
      "Epoch 8/10\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 0.1400 - val_loss: 0.1346\n",
      "Epoch 9/10\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 0.1372 - val_loss: 0.1326\n",
      "Epoch 10/10\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 0.1346 - val_loss: 0.1340\n",
      "Epoch 1/10\n",
      "120/120 [==============================] - 1s 4ms/step - loss: 0.1626 - val_loss: 0.1285\n",
      "Epoch 2/10\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 0.1503 - val_loss: 0.1285\n",
      "Epoch 3/10\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 0.1435 - val_loss: 0.1210\n",
      "Epoch 4/10\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 0.1407 - val_loss: 0.1252\n",
      "Epoch 5/10\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 0.1372 - val_loss: 0.1205\n",
      "Epoch 6/10\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 0.1350 - val_loss: 0.1190\n",
      "Epoch 7/10\n",
      "120/120 [==============================] - 1s 4ms/step - loss: 0.1326 - val_loss: 0.1198\n",
      "Epoch 8/10\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 0.1313 - val_loss: 0.1201\n",
      "Epoch 9/10\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 0.1288 - val_loss: 0.1190\n",
      "Epoch 10/10\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 0.1282 - val_loss: 0.1152\n",
      "Epoch 1/10\n",
      "120/120 [==============================] - 1s 6ms/step - loss: 0.1529 - val_loss: 0.1169\n",
      "Epoch 2/10\n",
      "120/120 [==============================] - 1s 4ms/step - loss: 0.1422 - val_loss: 0.1164\n",
      "Epoch 3/10\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 0.1371 - val_loss: 0.1129\n",
      "Epoch 4/10\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 0.1308 - val_loss: 0.1118\n",
      "Epoch 5/10\n",
      "120/120 [==============================] - 0s 4ms/step - loss: 0.1293 - val_loss: 0.1104\n",
      "Epoch 6/10\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 0.1280 - val_loss: 0.1148\n",
      "Epoch 7/10\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 0.1253 - val_loss: 0.1111\n",
      "Epoch 8/10\n",
      "120/120 [==============================] - 1s 4ms/step - loss: 0.1233 - val_loss: 0.1096\n",
      "Epoch 9/10\n",
      "120/120 [==============================] - 1s 4ms/step - loss: 0.1212 - val_loss: 0.1074\n",
      "Epoch 10/10\n",
      "120/120 [==============================] - 1s 6ms/step - loss: 0.1203 - val_loss: 0.1158\n",
      "Epoch 1/10\n",
      "120/120 [==============================] - 1s 6ms/step - loss: 0.1430 - val_loss: 0.1139\n",
      "Epoch 2/10\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 0.1327 - val_loss: 0.1033\n",
      "Epoch 3/10\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 0.1273 - val_loss: 0.1020\n",
      "Epoch 4/10\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 0.1244 - val_loss: 0.1000\n",
      "Epoch 5/10\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 0.1209 - val_loss: 0.1036\n",
      "Epoch 6/10\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 0.1190 - val_loss: 0.0984\n",
      "Epoch 7/10\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 0.1171 - val_loss: 0.0996\n",
      "Epoch 8/10\n",
      "120/120 [==============================] - 1s 6ms/step - loss: 0.1153 - val_loss: 0.0989\n",
      "Epoch 9/10\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 0.1141 - val_loss: 0.1005\n",
      "Epoch 10/10\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 0.1129 - val_loss: 0.1021\n",
      "Epoch 1/10\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 0.1373 - val_loss: 0.0959\n",
      "Epoch 2/10\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 0.1263 - val_loss: 0.0915\n",
      "Epoch 3/10\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 0.1206 - val_loss: 0.0898\n",
      "Epoch 4/10\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 0.1185 - val_loss: 0.0931\n",
      "Epoch 5/10\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 0.1179 - val_loss: 0.0919\n",
      "Epoch 6/10\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 0.1128 - val_loss: 0.0910\n",
      "Epoch 7/10\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 0.1122 - val_loss: 0.0892\n",
      "Epoch 8/10\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 0.1103 - val_loss: 0.0894\n",
      "Epoch 9/10\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 0.1083 - val_loss: 0.0886\n",
      "Epoch 10/10\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 0.1079 - val_loss: 0.0907\n"
     ]
    }
   ],
   "source": [
    "from small_rubiks_neural_networks import student_network\n",
    "import numpy as np\n",
    "import small_rubiks as rubiks\n",
    "from utils import dotdict\n",
    "from keras import regularizers\n",
    "\n",
    "params = dotdict({\n",
    "    \"residual_weights_reg\" : regularizers.l2(0),\n",
    "    \"residual_bias_reg\" : regularizers.l2(0),\n",
    "    \"relu_leak\" : 0.1,\n",
    "    \"residual_units\" : 100,\n",
    "    \"learning_rate\" : 0.001,\n",
    "    \"residual_layers\" : 4\n",
    "})\n",
    "\n",
    "my_student_network = student_network()\n",
    "my_student_network.build_state_network(params)\n",
    "\n",
    "n=4096\n",
    "rng = np.random.default_rng(seed=0)\n",
    "states = np.arange(24)[None,:].repeat(n,axis=0)\n",
    "\n",
    "for k in range(15):\n",
    "    for i in range(30):\n",
    "        _, states = rubiks.task_action(states,rng.choice(a=12,size=n))\n",
    "\n",
    "    actions = rng.choice(12,size=n)\n",
    "    _, next_states = rubiks.task_action(states,actions)\n",
    "    my_student_network.fit_state(states,actions,next_states,epochs = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = np.arange(24)[None,:]\n",
    "actions = np.array([0])\n",
    "prediction = my_student_network.predict_state(states,actions)\n",
    "prediction.reshape(1,24,6), prediction.reshape(1,24,6).argmax(axis=2), rubiks.start_coloring[rubiks.task_action(states,actions)[1][0]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ea0df938a2fb4e996541c96a685bf803a61ab029232d990df5cc8c7e24c9f388"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
